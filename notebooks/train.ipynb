{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ../logs_pocky_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0112 14:59:59.705018 4541277632 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/baselines/common/tf_util.py:53: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0112 14:59:59.706222 4541277632 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/baselines/common/tf_util.py:63: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0112 14:59:59.707085 4541277632 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/baselines/common/tf_util.py:70: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
      "\n",
      "W0112 14:59:59.710613 4541277632 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/baselines/common/misc_util.py:58: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "W0112 14:59:59.717421 4541277632 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/baselines/deepq/deepq.py:205: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0112 14:59:59.718208 4541277632 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/baselines/deepq/build_graph.py:176: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0112 14:59:59.723486 4541277632 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/baselines/common/input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0112 14:59:59.731817 4541277632 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/baselines/common/models.py:94: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W0112 15:00:00.533256 4541277632 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/baselines/deepq/build_graph.py:189: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 98       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | -4       |\n",
      "| steps                   | 163      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | -3.9     |\n",
      "| steps                   | 334      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 95       |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | -4.1     |\n",
      "| steps                   | 500      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | -4       |\n",
      "| steps                   | 675      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | -3.9     |\n",
      "| steps                   | 864      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | -4       |\n",
      "| steps                   | 1.05e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 88       |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | -3.7     |\n",
      "| steps                   | 1.22e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | -3.9     |\n",
      "| steps                   | 1.4e+03  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 84       |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | -3.7     |\n",
      "| steps                   | 1.58e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 1e+03    |\n",
      "| mean 100 episode reward | -4.1     |\n",
      "| steps                   | 1.77e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 80       |\n",
      "| episodes                | 1.1e+03  |\n",
      "| mean 100 episode reward | -3.9     |\n",
      "| steps                   | 1.95e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 78       |\n",
      "| episodes                | 1.2e+03  |\n",
      "| mean 100 episode reward | -3.9     |\n",
      "| steps                   | 2.16e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 76       |\n",
      "| episodes                | 1.3e+03  |\n",
      "| mean 100 episode reward | -3.8     |\n",
      "| steps                   | 2.35e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 74       |\n",
      "| episodes                | 1.4e+03  |\n",
      "| mean 100 episode reward | -3.9     |\n",
      "| steps                   | 2.58e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 72       |\n",
      "| episodes                | 1.5e+03  |\n",
      "| mean 100 episode reward | -3.7     |\n",
      "| steps                   | 2.81e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 70       |\n",
      "| episodes                | 1.6e+03  |\n",
      "| mean 100 episode reward | -3.8     |\n",
      "| steps                   | 3.04e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 68       |\n",
      "| episodes                | 1.7e+03  |\n",
      "| mean 100 episode reward | -3.7     |\n",
      "| steps                   | 3.23e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 66       |\n",
      "| episodes                | 1.8e+03  |\n",
      "| mean 100 episode reward | -3.9     |\n",
      "| steps                   | 3.43e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 64       |\n",
      "| episodes                | 1.9e+03  |\n",
      "| mean 100 episode reward | -3.7     |\n",
      "| steps                   | 3.67e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 61       |\n",
      "| episodes                | 2e+03    |\n",
      "| mean 100 episode reward | -3.7     |\n",
      "| steps                   | 3.91e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 59       |\n",
      "| episodes                | 2.1e+03  |\n",
      "| mean 100 episode reward | -3.9     |\n",
      "| steps                   | 4.13e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 56       |\n",
      "| episodes                | 2.2e+03  |\n",
      "| mean 100 episode reward | -3.6     |\n",
      "| steps                   | 4.4e+03  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 54       |\n",
      "| episodes                | 2.3e+03  |\n",
      "| mean 100 episode reward | -3.3     |\n",
      "| steps                   | 4.68e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 51       |\n",
      "| episodes                | 2.4e+03  |\n",
      "| mean 100 episode reward | -3.7     |\n",
      "| steps                   | 4.95e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 48       |\n",
      "| episodes                | 2.5e+03  |\n",
      "| mean 100 episode reward | -3.3     |\n",
      "| steps                   | 5.21e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 46       |\n",
      "| episodes                | 2.6e+03  |\n",
      "| mean 100 episode reward | -3.2     |\n",
      "| steps                   | 5.48e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 43       |\n",
      "| episodes                | 2.7e+03  |\n",
      "| mean 100 episode reward | -3.6     |\n",
      "| steps                   | 5.8e+03  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 40       |\n",
      "| episodes                | 2.8e+03  |\n",
      "| mean 100 episode reward | -3.3     |\n",
      "| steps                   | 6.12e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 36       |\n",
      "| episodes                | 2.9e+03  |\n",
      "| mean 100 episode reward | -3.5     |\n",
      "| steps                   | 6.47e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 33       |\n",
      "| episodes                | 3e+03    |\n",
      "| mean 100 episode reward | -3.3     |\n",
      "| steps                   | 6.82e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 29       |\n",
      "| episodes                | 3.1e+03  |\n",
      "| mean 100 episode reward | -3.3     |\n",
      "| steps                   | 7.22e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 24       |\n",
      "| episodes                | 3.2e+03  |\n",
      "| mean 100 episode reward | -2.9     |\n",
      "| steps                   | 7.66e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 20       |\n",
      "| episodes                | 3.3e+03  |\n",
      "| mean 100 episode reward | -2.8     |\n",
      "| steps                   | 8.13e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 15       |\n",
      "| episodes                | 3.4e+03  |\n",
      "| mean 100 episode reward | -2.6     |\n",
      "| steps                   | 8.67e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 8        |\n",
      "| episodes                | 3.5e+03  |\n",
      "| mean 100 episode reward | -3       |\n",
      "| steps                   | 9.38e+03 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: None -> -2.3\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3.6e+03  |\n",
      "| mean 100 episode reward | -1.9     |\n",
      "| steps                   | 1.17e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3.7e+03  |\n",
      "| mean 100 episode reward | 2.1      |\n",
      "| steps                   | 1.58e+04 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: -2.3 -> 5.0\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3.8e+03  |\n",
      "| mean 100 episode reward | 6.5      |\n",
      "| steps                   | 2.14e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3.9e+03  |\n",
      "| mean 100 episode reward | 7.6      |\n",
      "| steps                   | 2.77e+04 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 5.0 -> 9.0\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4e+03    |\n",
      "| mean 100 episode reward | 5.1      |\n",
      "| steps                   | 3.46e+04 |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from baselines import deepq\n",
    "from baselines import logger\n",
    "\n",
    "from gym_unity.envs import UnityEnv\n",
    "\n",
    "def callback(lcl, _glb):\n",
    "    # stop training if reward exceeds n\n",
    "    is_solved = lcl['t'] > 100 and sum(lcl['episode_rewards'][-101:-1]) / 100 >= 1000\n",
    "    return is_solved\n",
    "\n",
    "def main():\n",
    "    env_name = \"pocky_train\"\n",
    "    env = UnityEnv(\"../envs/\" + env_name, 17, use_visual=False)\n",
    "    logger.configure('../logs_' + env_name) # Ã‡hange to log in a different directory\n",
    "    act = deepq.learn(\n",
    "        env,\n",
    "        network='mlp',\n",
    "        lr=1e-3,\n",
    "        total_timesteps=100000,\n",
    "        buffer_size=5000,\n",
    "        exploration_fraction=0.1,\n",
    "        exploration_final_eps=0.02,\n",
    "        print_freq=100,\n",
    "        callback=callback,\n",
    "        dueling=True\n",
    "    )\n",
    "    print(\"models/{}.pkl\".format(env_name))\n",
    "    act.save(\"models/{}.pkl\".format(env_name))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
