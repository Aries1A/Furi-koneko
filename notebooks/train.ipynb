{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ../logs_pocky_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0111 03:59:27.764638 4684428736 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/baselines/common/tf_util.py:53: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0111 03:59:27.765742 4684428736 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/baselines/common/tf_util.py:63: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0111 03:59:27.766463 4684428736 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/baselines/common/tf_util.py:70: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
      "\n",
      "W0111 03:59:27.770383 4684428736 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/baselines/common/misc_util.py:58: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "W0111 03:59:27.779398 4684428736 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/baselines/deepq/deepq.py:205: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0111 03:59:27.780823 4684428736 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/baselines/deepq/build_graph.py:176: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0111 03:59:27.786824 4684428736 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/baselines/common/input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0111 03:59:27.798506 4684428736 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/baselines/common/models.py:94: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W0111 03:59:28.826059 4684428736 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/baselines/deepq/build_graph.py:189: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 98       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | -4.2     |\n",
      "| steps                   | 185      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | -4.1     |\n",
      "| steps                   | 363      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 94       |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | -3.8     |\n",
      "| steps                   | 530      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 93       |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | -4.2     |\n",
      "| steps                   | 712      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | -3.9     |\n",
      "| steps                   | 869      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 89       |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | -4       |\n",
      "| steps                   | 1.03e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 88       |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | -4.5     |\n",
      "| steps                   | 1.2e+03  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | -4.2     |\n",
      "| steps                   | 1.37e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 84       |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | -3.7     |\n",
      "| steps                   | 1.55e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 83       |\n",
      "| episodes                | 1e+03    |\n",
      "| mean 100 episode reward | -4       |\n",
      "| steps                   | 1.72e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 81       |\n",
      "| episodes                | 1.1e+03  |\n",
      "| mean 100 episode reward | -4       |\n",
      "| steps                   | 1.91e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 79       |\n",
      "| episodes                | 1.2e+03  |\n",
      "| mean 100 episode reward | -3.8     |\n",
      "| steps                   | 2.1e+03  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 77       |\n",
      "| episodes                | 1.3e+03  |\n",
      "| mean 100 episode reward | -3.6     |\n",
      "| steps                   | 2.28e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 75       |\n",
      "| episodes                | 1.4e+03  |\n",
      "| mean 100 episode reward | -4       |\n",
      "| steps                   | 2.49e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 73       |\n",
      "| episodes                | 1.5e+03  |\n",
      "| mean 100 episode reward | -3.8     |\n",
      "| steps                   | 2.71e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 71       |\n",
      "| episodes                | 1.6e+03  |\n",
      "| mean 100 episode reward | -3.6     |\n",
      "| steps                   | 2.93e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 68       |\n",
      "| episodes                | 1.7e+03  |\n",
      "| mean 100 episode reward | -3.6     |\n",
      "| steps                   | 3.18e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 66       |\n",
      "| episodes                | 1.8e+03  |\n",
      "| mean 100 episode reward | -3.4     |\n",
      "| steps                   | 3.37e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 64       |\n",
      "| episodes                | 1.9e+03  |\n",
      "| mean 100 episode reward | -4.3     |\n",
      "| steps                   | 3.6e+03  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 62       |\n",
      "| episodes                | 2e+03    |\n",
      "| mean 100 episode reward | -3.5     |\n",
      "| steps                   | 3.82e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 60       |\n",
      "| episodes                | 2.1e+03  |\n",
      "| mean 100 episode reward | -3.9     |\n",
      "| steps                   | 4.03e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 57       |\n",
      "| episodes                | 2.2e+03  |\n",
      "| mean 100 episode reward | -3.4     |\n",
      "| steps                   | 4.31e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 55       |\n",
      "| episodes                | 2.3e+03  |\n",
      "| mean 100 episode reward | -3.9     |\n",
      "| steps                   | 4.57e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 52       |\n",
      "| episodes                | 2.4e+03  |\n",
      "| mean 100 episode reward | -3.8     |\n",
      "| steps                   | 4.8e+03  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 50       |\n",
      "| episodes                | 2.5e+03  |\n",
      "| mean 100 episode reward | -3.9     |\n",
      "| steps                   | 5.1e+03  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 47       |\n",
      "| episodes                | 2.6e+03  |\n",
      "| mean 100 episode reward | -3.5     |\n",
      "| steps                   | 5.39e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 44       |\n",
      "| episodes                | 2.7e+03  |\n",
      "| mean 100 episode reward | -3.5     |\n",
      "| steps                   | 5.66e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 41       |\n",
      "| episodes                | 2.8e+03  |\n",
      "| mean 100 episode reward | -3.3     |\n",
      "| steps                   | 5.94e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 39       |\n",
      "| episodes                | 2.9e+03  |\n",
      "| mean 100 episode reward | -3.5     |\n",
      "| steps                   | 6.22e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 35       |\n",
      "| episodes                | 3e+03    |\n",
      "| mean 100 episode reward | -3.6     |\n",
      "| steps                   | 6.59e+03 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 31       |\n",
      "| episodes                | 3.1e+03  |\n",
      "| mean 100 episode reward | -3.5     |\n",
      "| steps                   | 6.99e+03 |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from baselines import deepq\n",
    "from baselines import logger\n",
    "\n",
    "from gym_unity.envs import UnityEnv\n",
    "\n",
    "def callback(lcl, _glb):\n",
    "    # stop training if reward exceeds n\n",
    "    is_solved = lcl['t'] > 100 and sum(lcl['episode_rewards'][-101:-1]) / 100 >= 1000\n",
    "    return is_solved\n",
    "\n",
    "def main():\n",
    "    env_name = \"pocky_train\"\n",
    "    env = UnityEnv(\"../envs/\" + env_name, 17, use_visual=False)\n",
    "    logger.configure('../logs_' + env_name) # Çhange to log in a different directory\n",
    "    act = deepq.learn(\n",
    "        env,\n",
    "        network='mlp',\n",
    "        lr=1e-3,\n",
    "        total_timesteps=100000,\n",
    "        buffer_size=5000,\n",
    "        exploration_fraction=0.1,\n",
    "        exploration_final_eps=0.02,\n",
    "        print_freq=100,\n",
    "        callback=callback,\n",
    "        dueling=True\n",
    "    )\n",
    "    print(\"models/{}.pkl\".format(env_name))\n",
    "    act.save(\"models/{}.pkl\".format(env_name))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
